{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five subreddits of interest\n",
    "subreddit_1 = 'burgers'\n",
    "subreddit_2 = 'hotdogs'\n",
    "subreddit_3 = 'Pizza'\n",
    "subreddit_4 = 'pasta'\n",
    "subreddit_5 = 'sushi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base url\n",
    "base_url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present utc: 3/20/21 at 9:46 PM\n",
    "present_utc = 1616291202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a dataframe with 200 images from a subreddit\n",
    "def images_200(subreddit, before):\n",
    "    # Set parameters\n",
    "    params_100 = {\n",
    "    'subreddit': subreddit,\n",
    "    'size': 100,\n",
    "    'before': present_utc\n",
    "    }\n",
    "    \n",
    "    # Request content\n",
    "    res_100 = requests.get(base_url, params_100).json()\n",
    "    posts_100 = res_100['data']\n",
    "    \n",
    "    # Last utc\n",
    "    last_utc = posts_100[-1]['created_utc']\n",
    "    \n",
    "    # Create a dataframe with the first 100 images\n",
    "    df_100 = pd.DataFrame(posts_100)[['subreddit', 'title', 'url']]\n",
    "    \n",
    "    # Use a for loop to retrieve 200 images\n",
    "    for i in range(1):\n",
    "        params_200 = {\n",
    "            'subreddit': subreddit,\n",
    "            'size': 100,\n",
    "            'before': last_utc\n",
    "        }\n",
    "        res_200 = requests.get(base_url, params_200).json()\n",
    "        posts_200 = res_200['data']\n",
    "        last_utc = posts_200[-1]['created_utc']\n",
    "        df_200 = pd.DataFrame(posts_200)[['subreddit', 'title', 'url']]\n",
    "        df_100 = pd.concat([df_100, df_200])\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Return a dataframe\n",
    "    return df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a dataframe with 200 images for a list of 5 subreddits\n",
    "subreddit_list = [subreddit_1, subreddit_2, subreddit_3, subreddit_4, subreddit_5]\n",
    "\n",
    "def subreddit_images(my_list, before):\n",
    "    for subreddit_name in my_list:\n",
    "        subreddit_df = images_200(subreddit_name, before)\n",
    "        if subreddit_name == subreddit_1:\n",
    "            combined_df = subreddit_df\n",
    "        else:\n",
    "            combined_df = pd.concat([combined_df, subreddit_df])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check code execution\n",
    "df = subreddit_images(subreddit_list, present_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Pimento cheese falafel burger for St. Paddies ...</td>\n",
       "      <td>https://i.redd.it/2fbas0e9gmn61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Bob Belcher</td>\n",
       "      <td>https://i.redd.it/t9wvrc1u6mn61.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Airfried FROZEN BURGER PATTIESüçî. Tips and tricks</td>\n",
       "      <td>https://youtu.be/q0gfAKxNex4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Double Smashburger on Homemade Buns</td>\n",
       "      <td>https://i.redd.it/vctmwmk78ln61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Double Cheeseburger, Bacon, Pickles, Grilled &amp;...</td>\n",
       "      <td>https://i.redd.it/o4fjy09zokn61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Today's dinner</td>\n",
       "      <td>https://i.redd.it/aqy98t1k5wk61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Great Sushi Dragon</td>\n",
       "      <td>https://i.redd.it/h5s5j7q2vvk61.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Fried sushi roll (hosomaki)</td>\n",
       "      <td>https://i.redd.it/dqxyj9mztvk61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>sushi</td>\n",
       "      <td>How To Make Sushi At Home Without A Rice Cooke...</td>\n",
       "      <td>https://youtu.be/Sk9klgB571M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>sushi</td>\n",
       "      <td>My first attempt at making sushi!</td>\n",
       "      <td>https://i.redd.it/xx5ogy2ogvk61.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              title  \\\n",
       "0    burgers  Pimento cheese falafel burger for St. Paddies ...   \n",
       "1    burgers                                        Bob Belcher   \n",
       "2    burgers   Airfried FROZEN BURGER PATTIESüçî. Tips and tricks   \n",
       "3    burgers                Double Smashburger on Homemade Buns   \n",
       "4    burgers  Double Cheeseburger, Bacon, Pickles, Grilled &...   \n",
       "..       ...                                                ...   \n",
       "95     sushi                                     Today's dinner   \n",
       "96     sushi                                 Great Sushi Dragon   \n",
       "97     sushi                        Fried sushi roll (hosomaki)   \n",
       "98     sushi  How To Make Sushi At Home Without A Rice Cooke...   \n",
       "99     sushi                  My first attempt at making sushi!   \n",
       "\n",
       "                                    url  \n",
       "0   https://i.redd.it/2fbas0e9gmn61.jpg  \n",
       "1   https://i.redd.it/t9wvrc1u6mn61.png  \n",
       "2          https://youtu.be/q0gfAKxNex4  \n",
       "3   https://i.redd.it/vctmwmk78ln61.jpg  \n",
       "4   https://i.redd.it/o4fjy09zokn61.jpg  \n",
       "..                                  ...  \n",
       "95  https://i.redd.it/aqy98t1k5wk61.jpg  \n",
       "96  https://i.redd.it/h5s5j7q2vvk61.png  \n",
       "97  https://i.redd.it/dqxyj9mztvk61.jpg  \n",
       "98         https://youtu.be/Sk9klgB571M  \n",
       "99  https://i.redd.it/xx5ogy2ogvk61.jpg  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    200\n",
       "Pizza      200\n",
       "pasta      200\n",
       "sushi      200\n",
       "hotdogs    200\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts for the 'subreddit' column\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls that are not .jpg or .png\n",
    "clean_df = df.loc[df['url'].str.contains('jpg') | df['url'].str.contains('png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    143\n",
       "hotdogs    125\n",
       "sushi      123\n",
       "Pizza       99\n",
       "pasta       94\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts again\n",
    "clean_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows from dataframe\n",
    "clean_df = clean_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    143\n",
       "hotdogs    125\n",
       "sushi      123\n",
       "Pizza       99\n",
       "pasta       94\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts again\n",
    "clean_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** Each class has 1000 or more images available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced: https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced: https://www.kite.com/python/answers/how-to-catch-an-httperror-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import urllib.request\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://i.redd.it/2fbas0e9gmn61.jpg',\n",
       " 'https://i.redd.it/t9wvrc1u6mn61.png',\n",
       " 'https://i.redd.it/vctmwmk78ln61.jpg',\n",
       " 'https://i.redd.it/o4fjy09zokn61.jpg',\n",
       " 'https://i.redd.it/xepjccl7hjn61.jpg']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of urls\n",
    "url_list = list(clean_df['url'])\n",
    "url_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each url as either a .jpg or .png file\n",
    "jpg_counter = 1\n",
    "png_counter = 1\n",
    "for url in url_list:\n",
    "    if 'jpg' in url:\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, f'../images/image_{jpg_counter}.jpg')\n",
    "            jpg_counter += 1\n",
    "        except urllib.error.HTTPError:\n",
    "            pass\n",
    "    elif 'png' in url:\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, f'../images/image_{png_counter}.png')\n",
    "            png_counter += 1\n",
    "        except urllib.error.HTTPError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of images stored\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Progress Update:** All the image data has been successfully collected and stored as .png and .jpg files! Incorrect images still need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Incorrectly Classified Images from the Dataset\n",
    "Given the source of the images, not all images were classified properly in the dataset. The following images have been removed manually due to blatant misclassification errors:\n",
    "- image_1.png\n",
    "- image_57.jpg\n",
    "- image_2.png\n",
    "- image_104.jpg\n",
    "- image_107.jpg\n",
    "- image_122.jpg\n",
    "- image_3.png\n",
    "- image_4.png\n",
    "- image_147.jpg\n",
    "- image_150.jpg\n",
    "- image_153.jpg\n",
    "- image_154.jpg\n",
    "- image_165.jpg\n",
    "- image_5.png\n",
    "- image_186.jpg\n",
    "- image_187.jpg\n",
    "- image_194.jpg\n",
    "- image_220.jpg\n",
    "- image_224.jpg\n",
    "- image_331.jpg\n",
    "- image_10.png\n",
    "- image_347.jpg\n",
    "- image_350.jpg\n",
    "- image_362.jpg\n",
    "- image_396.jpg\n",
    "- image_449.jpg\n",
    "- image_13.png\n",
    "- image_466.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five subreddits of interest\n",
    "subreddit_1 = 'burgers'\n",
    "subreddit_2 = 'hotdogs'\n",
    "subreddit_3 = 'Pizza'\n",
    "subreddit_4 = 'pasta'\n",
    "subreddit_5 = 'sushi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base url\n",
    "base_url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present utc: 3/20/21 at 9:46 PM\n",
    "present_utc = 1616291202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a dataframe with 200 images from a subreddit\n",
    "def images_200(subreddit, before):\n",
    "    # Set parameters\n",
    "    params_100 = {\n",
    "    'subreddit': subreddit,\n",
    "    'size': 100,\n",
    "    'before': present_utc\n",
    "    }\n",
    "    \n",
    "    # Request content\n",
    "    res_100 = requests.get(base_url, params_100).json()\n",
    "    posts_100 = res_100['data']\n",
    "    \n",
    "    # Last utc\n",
    "    last_utc = posts_100[-1]['created_utc']\n",
    "    \n",
    "    # Create a dataframe with the first 100 images\n",
    "    df_100 = pd.DataFrame(posts_100)[['subreddit', 'title', 'url']]\n",
    "    \n",
    "    # Use a for loop to retrieve 200 images\n",
    "    for i in range(1):\n",
    "        params_200 = {\n",
    "            'subreddit': subreddit,\n",
    "            'size': 100,\n",
    "            'before': last_utc\n",
    "        }\n",
    "        res_200 = requests.get(base_url, params_200).json()\n",
    "        posts_200 = res_200['data']\n",
    "        last_utc = posts_200[-1]['created_utc']\n",
    "        df_200 = pd.DataFrame(posts_200)[['subreddit', 'title', 'url']]\n",
    "        df_100 = pd.concat([df_100, df_200])\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Return a dataframe\n",
    "    return df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a dataframe with 200 images for a list of 5 subreddits\n",
    "subreddit_list = [subreddit_1, subreddit_2, subreddit_3, subreddit_4, subreddit_5]\n",
    "\n",
    "def subreddit_images(my_list, before):\n",
    "    for subreddit_name in my_list:\n",
    "        subreddit_df = images_200(subreddit_name, before)\n",
    "        if subreddit_name == subreddit_1:\n",
    "            combined_df = subreddit_df\n",
    "        else:\n",
    "            combined_df = pd.concat([combined_df, subreddit_df])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check code execution\n",
    "df = subreddit_images(subreddit_list, present_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Pimento cheese falafel burger for St. Paddies ...</td>\n",
       "      <td>https://i.redd.it/2fbas0e9gmn61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Bob Belcher</td>\n",
       "      <td>https://i.redd.it/t9wvrc1u6mn61.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Airfried FROZEN BURGER PATTIESüçî. Tips and tricks</td>\n",
       "      <td>https://youtu.be/q0gfAKxNex4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Double Smashburger on Homemade Buns</td>\n",
       "      <td>https://i.redd.it/vctmwmk78ln61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Double Cheeseburger, Bacon, Pickles, Grilled &amp;...</td>\n",
       "      <td>https://i.redd.it/o4fjy09zokn61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Today's dinner</td>\n",
       "      <td>https://i.redd.it/aqy98t1k5wk61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Great Sushi Dragon</td>\n",
       "      <td>https://i.redd.it/h5s5j7q2vvk61.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Fried sushi roll (hosomaki)</td>\n",
       "      <td>https://i.redd.it/dqxyj9mztvk61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>sushi</td>\n",
       "      <td>How To Make Sushi At Home Without A Rice Cooke...</td>\n",
       "      <td>https://youtu.be/Sk9klgB571M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>sushi</td>\n",
       "      <td>My first attempt at making sushi!</td>\n",
       "      <td>https://i.redd.it/xx5ogy2ogvk61.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              title  \\\n",
       "0    burgers  Pimento cheese falafel burger for St. Paddies ...   \n",
       "1    burgers                                        Bob Belcher   \n",
       "2    burgers   Airfried FROZEN BURGER PATTIESüçî. Tips and tricks   \n",
       "3    burgers                Double Smashburger on Homemade Buns   \n",
       "4    burgers  Double Cheeseburger, Bacon, Pickles, Grilled &...   \n",
       "..       ...                                                ...   \n",
       "95     sushi                                     Today's dinner   \n",
       "96     sushi                                 Great Sushi Dragon   \n",
       "97     sushi                        Fried sushi roll (hosomaki)   \n",
       "98     sushi  How To Make Sushi At Home Without A Rice Cooke...   \n",
       "99     sushi                  My first attempt at making sushi!   \n",
       "\n",
       "                                    url  \n",
       "0   https://i.redd.it/2fbas0e9gmn61.jpg  \n",
       "1   https://i.redd.it/t9wvrc1u6mn61.png  \n",
       "2          https://youtu.be/q0gfAKxNex4  \n",
       "3   https://i.redd.it/vctmwmk78ln61.jpg  \n",
       "4   https://i.redd.it/o4fjy09zokn61.jpg  \n",
       "..                                  ...  \n",
       "95  https://i.redd.it/aqy98t1k5wk61.jpg  \n",
       "96  https://i.redd.it/h5s5j7q2vvk61.png  \n",
       "97  https://i.redd.it/dqxyj9mztvk61.jpg  \n",
       "98         https://youtu.be/Sk9klgB571M  \n",
       "99  https://i.redd.it/xx5ogy2ogvk61.jpg  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pasta      200\n",
       "sushi      200\n",
       "Pizza      200\n",
       "hotdogs    200\n",
       "burgers    200\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts for the 'subreddit' column\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls that are not .jpg or .png\n",
    "clean_df = df.loc[df['url'].str.contains('jpg') | df['url'].str.contains('png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    143\n",
       "hotdogs    125\n",
       "sushi      123\n",
       "Pizza       99\n",
       "pasta       94\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts again\n",
    "clean_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows from dataframe\n",
    "clean_df = clean_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    143\n",
       "hotdogs    125\n",
       "sushi      123\n",
       "Pizza       99\n",
       "pasta       94\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts again\n",
    "clean_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** Each class has 1000 or more images available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced: https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced: https://www.kite.com/python/answers/how-to-catch-an-httperror-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import urllib.request\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of urls for each subreddit\n",
    "burgers_url = list(clean_df.loc[clean_df['subreddit'] == 'burgers']['url'])\n",
    "hotdogs_url = list(clean_df.loc[clean_df['subreddit'] == 'hotdogs']['url'])\n",
    "pizza_url = list(clean_df.loc[clean_df['subreddit'] == 'Pizza']['url'])\n",
    "pasta_url = list(clean_df.loc[clean_df['subreddit'] == 'pasta']['url'])\n",
    "sushi_url = list(clean_df.loc[clean_df['subreddit'] == 'sushi']['url'])\n",
    "\n",
    "# Create a list containing the urls for each food class\n",
    "food_url = [burgers_url, hotdogs_url, pizza_url, pasta_url, sushi_url]\n",
    "\n",
    "# Create a list of strings describing the food class\n",
    "food_list = ['burgers', 'hotdogs', 'pizza', 'pasta', 'sushi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that saves each url as either a .jpg or .png file\n",
    "def url_saver(url_list, food_name):\n",
    "    jpg_counter = 1\n",
    "    png_counter = 1\n",
    "    total_count = 0\n",
    "    for url in url_list:\n",
    "        if 'jpg' in url:\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, f'../images/{food_name}/{food_name}_{jpg_counter}.jpg')\n",
    "                jpg_counter += 1\n",
    "                total_count += 1\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "        elif 'png' in url:\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, f'../images/{food_name}/{food_name}_{png_counter}.png')\n",
    "                png_counter += 1\n",
    "                total_count += 1\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "    print(f'The total number of images for {food_name} is: {total_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of images for burgers is: 124\n",
      "The total number of images for hotdogs is: 103\n",
      "The total number of images for pizza is: 87\n",
      "The total number of images for pasta is: 77\n",
      "The total number of images for sushi is: 103\n"
     ]
    }
   ],
   "source": [
    "# Save all images by iterating over a for loop\n",
    "my_index = 0\n",
    "for food in food_url:\n",
    "    url_saver(food, food_list[my_index])\n",
    "    my_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Progress Update:** All the image data has been successfully collected and stored as .png and .jpg files! Incorrect images still need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Incorrectly Classified Images from the Dataset\n",
    "Given the source of the images, not all images were classified properly in the dataset. The following images have been removed manually due to blatant misclassification errors:\n",
    "- Burgers: burgers_1.png, burgers_2.png, burgers_104.jpg, burgers_107.jpg, burgers_122.jpg\n",
    "- Hot dogs: hotdogs_27.jpg, hotdogs_30.jpg, hotdogs_31.jpg, hotdogs_42.jpg, hotdogs_66.jpg, hotdogs_71.jpg, hotdogs_88.jpg\n",
    "- Pizza: pizza_1.jpg\n",
    "- Pasta: pasta_3.png, pasta_23.jpg, pasta_39.jpg, pasta_54.jpg\n",
    "- Sushi: sushi_14.jpg, sushi_3.png, sushi_84.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

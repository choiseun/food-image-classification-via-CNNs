{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five subreddits of interest\n",
    "subreddit_1 = 'burgers'\n",
    "subreddit_2 = 'hotdogs'\n",
    "subreddit_3 = 'Pizza'\n",
    "subreddit_4 = 'pasta'\n",
    "subreddit_5 = 'sushi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base url\n",
    "base_url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present utc: 3/20/21 at 9:46 PM\n",
    "present_utc = 1616291202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a dataframe with 500 images from a subreddit\n",
    "def images_200(subreddit, before):\n",
    "    # Set parameters\n",
    "    params_100 = {\n",
    "    'subreddit': subreddit,\n",
    "    'size': 100,\n",
    "    'before': present_utc\n",
    "    }\n",
    "    \n",
    "    # Request content\n",
    "    res_100 = requests.get(base_url, params_100).json()\n",
    "    posts_100 = res_100['data']\n",
    "    \n",
    "    # Last utc\n",
    "    last_utc = posts_100[-1]['created_utc']\n",
    "    \n",
    "    # Create a dataframe with the first 100 images\n",
    "    df_100 = pd.DataFrame(posts_100)[['subreddit', 'title', 'url']]\n",
    "    \n",
    "    # Use a for loop to retrieve 200 images\n",
    "    for i in range(5):\n",
    "        params_200 = {\n",
    "            'subreddit': subreddit,\n",
    "            'size': 100,\n",
    "            'before': last_utc\n",
    "        }\n",
    "        res_200 = requests.get(base_url, params_200).json()\n",
    "        posts_200 = res_200['data']\n",
    "        last_utc = posts_200[-1]['created_utc']\n",
    "        df_200 = pd.DataFrame(posts_200)[['subreddit', 'title', 'url']]\n",
    "        df_100 = pd.concat([df_100, df_200])\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Return a dataframe\n",
    "    return df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a dataframe with 200 images for a list of 5 subreddits\n",
    "subreddit_list = [subreddit_1, subreddit_2, subreddit_3, subreddit_4, subreddit_5]\n",
    "\n",
    "def subreddit_images(my_list, before):\n",
    "    for subreddit_name in my_list:\n",
    "        subreddit_df = images_200(subreddit_name, before)\n",
    "        if subreddit_name == subreddit_1:\n",
    "            combined_df = subreddit_df\n",
    "        else:\n",
    "            combined_df = pd.concat([combined_df, subreddit_df])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check code execution\n",
    "df = subreddit_images(subreddit_list, present_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Pimento cheese falafel burger for St. Paddies ...</td>\n",
       "      <td>https://i.redd.it/2fbas0e9gmn61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Bob Belcher</td>\n",
       "      <td>https://i.redd.it/t9wvrc1u6mn61.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Airfried FROZEN BURGER PATTIESüçî. Tips and tricks</td>\n",
       "      <td>https://youtu.be/q0gfAKxNex4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Double Smashburger on Homemade Buns</td>\n",
       "      <td>https://i.redd.it/vctmwmk78ln61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Double Cheeseburger, Bacon, Pickles, Grilled &amp;...</td>\n",
       "      <td>https://i.redd.it/o4fjy09zokn61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>sushi</td>\n",
       "      <td>üî• Is flaming sushi a thing where you‚Äôre at? üî• ...</td>\n",
       "      <td>https://www.reddit.com/gallery/lcc9ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>sushi</td>\n",
       "      <td>These Soy Sauce Dishes Bring an Image to Life ...</td>\n",
       "      <td>https://puloh.com/blog/these-soy-sauce-dishes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Anyone craving for sushi today?</td>\n",
       "      <td>https://i.redd.it/ip4y94ckeef61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Does anyone know how to cure ikura (or any fis...</td>\n",
       "      <td>https://www.reddit.com/r/sushi/comments/lc7olk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>sushi</td>\n",
       "      <td>First homemade sushi platter</td>\n",
       "      <td>https://i.redd.it/5ss2b6i10ef61.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              title  \\\n",
       "0    burgers  Pimento cheese falafel burger for St. Paddies ...   \n",
       "1    burgers                                        Bob Belcher   \n",
       "2    burgers   Airfried FROZEN BURGER PATTIESüçî. Tips and tricks   \n",
       "3    burgers                Double Smashburger on Homemade Buns   \n",
       "4    burgers  Double Cheeseburger, Bacon, Pickles, Grilled &...   \n",
       "..       ...                                                ...   \n",
       "95     sushi  üî• Is flaming sushi a thing where you‚Äôre at? üî• ...   \n",
       "96     sushi  These Soy Sauce Dishes Bring an Image to Life ...   \n",
       "97     sushi                    Anyone craving for sushi today?   \n",
       "98     sushi  Does anyone know how to cure ikura (or any fis...   \n",
       "99     sushi                       First homemade sushi platter   \n",
       "\n",
       "                                                  url  \n",
       "0                 https://i.redd.it/2fbas0e9gmn61.jpg  \n",
       "1                 https://i.redd.it/t9wvrc1u6mn61.png  \n",
       "2                        https://youtu.be/q0gfAKxNex4  \n",
       "3                 https://i.redd.it/vctmwmk78ln61.jpg  \n",
       "4                 https://i.redd.it/o4fjy09zokn61.jpg  \n",
       "..                                                ...  \n",
       "95              https://www.reddit.com/gallery/lcc9ko  \n",
       "96  https://puloh.com/blog/these-soy-sauce-dishes-...  \n",
       "97                https://i.redd.it/ip4y94ckeef61.jpg  \n",
       "98  https://www.reddit.com/r/sushi/comments/lc7olk...  \n",
       "99                https://i.redd.it/5ss2b6i10ef61.jpg  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    600\n",
       "hotdogs    600\n",
       "Pizza      600\n",
       "pasta      600\n",
       "sushi      600\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts for the 'subreddit' column\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls that are not .jpg or .png\n",
    "clean_df = df.loc[df['url'].str.contains('jpg') | df['url'].str.contains('png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    468\n",
       "hotdogs    380\n",
       "sushi      351\n",
       "Pizza      312\n",
       "pasta      272\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts again\n",
    "clean_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows from dataframe\n",
    "clean_df = clean_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    468\n",
       "hotdogs    380\n",
       "sushi      351\n",
       "Pizza      311\n",
       "pasta      272\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts again\n",
    "clean_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** Each class has 1000 or more images available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced: https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced: https://www.kite.com/python/answers/how-to-catch-an-httperror-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import urllib.request\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of urls for each subreddit\n",
    "burgers_url = list(clean_df.loc[clean_df['subreddit'] == 'burgers']['url'])\n",
    "hotdogs_url = list(clean_df.loc[clean_df['subreddit'] == 'hotdogs']['url'])\n",
    "pizza_url = list(clean_df.loc[clean_df['subreddit'] == 'Pizza']['url'])\n",
    "pasta_url = list(clean_df.loc[clean_df['subreddit'] == 'pasta']['url'])\n",
    "sushi_url = list(clean_df.loc[clean_df['subreddit'] == 'sushi']['url'])\n",
    "\n",
    "# Create a list containing the urls for each food class\n",
    "food_url = [burgers_url, hotdogs_url, pizza_url, pasta_url, sushi_url]\n",
    "\n",
    "# Create a list of strings describing the food class\n",
    "food_list = ['burgers', 'hotdogs', 'pizza', 'pasta', 'sushi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that saves each url as either a .jpg or .png file\n",
    "def url_saver(url_list, food_name):\n",
    "    jpg_counter = 1\n",
    "    png_counter = 1\n",
    "    total_count = 0\n",
    "    for url in url_list:\n",
    "        if 'jpg' in url:\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, f'../images/{food_name}/{food_name}_{jpg_counter}.jpg')\n",
    "                jpg_counter += 1\n",
    "                total_count += 1\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "        elif 'png' in url:\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, f'../images/{food_name}/{food_name}_{png_counter}.png')\n",
    "                png_counter += 1\n",
    "                total_count += 1\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "    print(f'The total number of images for {food_name} is: {total_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of images for burgers is: 370\n",
      "The total number of images for hotdogs is: 332\n",
      "The total number of images for pizza is: 278\n",
      "The total number of images for pasta is: 231\n",
      "The total number of images for sushi is: 291\n"
     ]
    }
   ],
   "source": [
    "# Save all images by iterating over a for loop\n",
    "my_index = 0\n",
    "for food in food_url:\n",
    "    url_saver(food, food_list[my_index])\n",
    "    my_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Progress Update:** All the image data has been successfully collected and stored as .png and .jpg files! Incorrect images still need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Incorrectly Classified Images from the Dataset\n",
    "Given the source of the images, not all images were classified properly in the dataset. The following images have been removed manually due to blatant misclassification errors:\n",
    "- Burgers: burgers_1.png, burgers_2.png, burgers_104.jpg, burgers_107.jpg, burgers_122.jpg\n",
    "- Hot dogs: hotdogs_27.jpg, hotdogs_30.jpg, hotdogs_31.jpg, hotdogs_42.jpg, hotdogs_66.jpg, hotdogs_71.jpg, hotdogs_88.jpg\n",
    "- Pizza: pizza_1.jpg\n",
    "- Pasta: pasta_3.png, pasta_23.jpg, pasta_39.jpg, pasta_54.jpg\n",
    "- Sushi: sushi_14.jpg, sushi_3.png, sushi_84.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

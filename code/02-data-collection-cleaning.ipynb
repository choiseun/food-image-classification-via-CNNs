{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for data collection\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Imports for loading image data\n",
    "import urllib.request\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddits of interest\n",
    "subreddit_1 = 'burgers'\n",
    "subreddit_2 = 'hotdogs'\n",
    "subreddit_3 = 'Pizza'\n",
    "subreddit_4 = 'tacos'\n",
    "subreddit_5 = 'sushi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base url\n",
    "base_url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present utc: 4/10/21 at 12:04 PM\n",
    "present_utc = 1618070648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a dataframe with n images from a subreddit\n",
    "def n_images(subreddit, before):\n",
    "    \n",
    "    # Set parameters\n",
    "    params = {\n",
    "    'subreddit': subreddit,\n",
    "    'size': 100,\n",
    "    'before': present_utc\n",
    "    }\n",
    "    \n",
    "    # Request content\n",
    "    res = requests.get(base_url, params).json()\n",
    "    posts = res['data']\n",
    "    \n",
    "    # Last utc\n",
    "    last_utc = posts[-1]['created_utc']\n",
    "    \n",
    "    # Create a dataframe with the first 100 images\n",
    "    df_og = pd.DataFrame(posts)[['subreddit', 'title', 'url']]\n",
    "    \n",
    "    # Use a for loop to retrieve more images - to specify, update 'range(n)'\n",
    "    for i in range(5):\n",
    "        params_n = {\n",
    "            'subreddit': subreddit,\n",
    "            'size': 100,\n",
    "            'before': last_utc\n",
    "        }\n",
    "        res_n = requests.get(base_url, params_n).json()\n",
    "        posts_n = res_n['data']\n",
    "        last_utc = posts_n[-1]['created_utc']\n",
    "        df_n = pd.DataFrame(posts_n)[['subreddit', 'title', 'url']]\n",
    "        df_og = pd.concat([df_og, df_n])\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Return a dataframe\n",
    "    return df_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subreddits\n",
    "subreddit_list = [subreddit_1, subreddit_2, subreddit_3, subreddit_4, subreddit_5]\n",
    "\n",
    "# Define a function that returns a dataframe with n images for a list of subreddits\n",
    "def subreddit_images(my_list, before):\n",
    "    for subreddit_name in my_list:\n",
    "        subreddit_df = n_images(subreddit_name, before)\n",
    "        if subreddit_name == subreddit_1:\n",
    "            combined_df = subreddit_df\n",
    "        else:\n",
    "            combined_df = pd.concat([combined_df, subreddit_df])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Am I missing something? It happened to me today.</td>\n",
       "      <td>https://i.redd.it/6xduxhnen6s61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Didn‚Äôt have all the ingredients that I would l...</td>\n",
       "      <td>https://i.redd.it/2e1v90z436s61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Turkey burgers for mom and toddler, beef for dad</td>\n",
       "      <td>https://i.redd.it/f9iedd5pn1s61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Smash burger with cheese and onions.</td>\n",
       "      <td>https://i.redd.it/1e3psut8m1s61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burgers</td>\n",
       "      <td>Homemade Shake Shack Double SmokeShack</td>\n",
       "      <td>https://i.imgur.com/tGLbFjS.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Cowboy roll- Shrimp tempura, spicy crab, tuna ...</td>\n",
       "      <td>https://i.redd.it/99ptxel8ipa61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Tiger Roll wants to tell you something! [Art]</td>\n",
       "      <td>https://i.imgur.com/goWiZhY.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Hamachi, Toro, chutoro and otoroüëåüèº</td>\n",
       "      <td>https://www.reddit.com/gallery/kuu1qw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>sushi</td>\n",
       "      <td>Discount sushi, plus a discount NFL Team üëç</td>\n",
       "      <td>https://i.redd.it/sfc476nz2ma61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>sushi</td>\n",
       "      <td>First attempt at sushi! What was everyone‚Äôs be...</td>\n",
       "      <td>https://i.redd.it/lh47pzrllla61.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              title  \\\n",
       "0    burgers   Am I missing something? It happened to me today.   \n",
       "1    burgers  Didn‚Äôt have all the ingredients that I would l...   \n",
       "2    burgers   Turkey burgers for mom and toddler, beef for dad   \n",
       "3    burgers               Smash burger with cheese and onions.   \n",
       "4    burgers             Homemade Shake Shack Double SmokeShack   \n",
       "..       ...                                                ...   \n",
       "95     sushi  Cowboy roll- Shrimp tempura, spicy crab, tuna ...   \n",
       "96     sushi      Tiger Roll wants to tell you something! [Art]   \n",
       "97     sushi                 Hamachi, Toro, chutoro and otoroüëåüèº   \n",
       "98     sushi         Discount sushi, plus a discount NFL Team üëç   \n",
       "99     sushi  First attempt at sushi! What was everyone‚Äôs be...   \n",
       "\n",
       "                                      url  \n",
       "0     https://i.redd.it/6xduxhnen6s61.jpg  \n",
       "1     https://i.redd.it/2e1v90z436s61.jpg  \n",
       "2     https://i.redd.it/f9iedd5pn1s61.jpg  \n",
       "3     https://i.redd.it/1e3psut8m1s61.jpg  \n",
       "4         https://i.imgur.com/tGLbFjS.jpg  \n",
       "..                                    ...  \n",
       "95    https://i.redd.it/99ptxel8ipa61.jpg  \n",
       "96        https://i.imgur.com/goWiZhY.png  \n",
       "97  https://www.reddit.com/gallery/kuu1qw  \n",
       "98    https://i.redd.it/sfc476nz2ma61.jpg  \n",
       "99    https://i.redd.it/lh47pzrllla61.jpg  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe\n",
    "df = subreddit_images(subreddit_list, present_utc)\n",
    "\n",
    "# Check code execution\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    600\n",
       "sushi      600\n",
       "hotdogs    600\n",
       "Pizza      600\n",
       "tacos      600\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts for the 'subreddit' column\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls that are not .jpg or .png\n",
    "clean_df = df.loc[df['url'].str.contains('jpg') | df['url'].str.contains('png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows from dataframe\n",
    "clean_df = clean_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burgers    445\n",
       "hotdogs    386\n",
       "sushi      355\n",
       "tacos      349\n",
       "Pizza      324\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts again\n",
    "clean_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of urls for each subreddit\n",
    "burgers_url = list(clean_df.loc[clean_df['subreddit'] == 'burgers']['url'])\n",
    "hotdogs_url = list(clean_df.loc[clean_df['subreddit'] == 'hotdogs']['url'])\n",
    "pizza_url = list(clean_df.loc[clean_df['subreddit'] == 'Pizza']['url'])\n",
    "tacos_url = list(clean_df.loc[clean_df['subreddit'] == 'tacos']['url'])\n",
    "sushi_url = list(clean_df.loc[clean_df['subreddit'] == 'sushi']['url'])\n",
    "\n",
    "# Create a list containing the urls for each food class\n",
    "food_url = [burgers_url, hotdogs_url, pizza_url, tacos_url, sushi_url]\n",
    "\n",
    "# Create a list of strings describing the food class\n",
    "food_list = ['burgers', 'hotdogs', 'pizza', 'tacos', 'sushi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced: https://stackoverflow.com/questions/1274405/how-to-create-new-folder\n",
    "# Define a function that makes a new directory for each food class\n",
    "def new_directory(food_class):\n",
    "    for each in food_class:\n",
    "        new_file = f'../images/{each}'\n",
    "        if not os.path.exists(new_file):\n",
    "            os.makedirs(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory for each food class\n",
    "new_directory(food_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced: https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know\n",
    "# Referenced: https://www.kite.com/python/answers/how-to-catch-an-httperror-in-python\n",
    "# Define a function that saves each url as either a .jpg or .png file\n",
    "def url_saver(url_list, food_name):\n",
    "    jpg_counter = 1\n",
    "    png_counter = 1\n",
    "    total_count = 0\n",
    "    for url in url_list:\n",
    "        if 'jpg' in url:\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, f'../images/{food_name}/{food_name}_{jpg_counter}.jpg')\n",
    "                jpg_counter += 1\n",
    "                total_count += 1\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "        elif 'png' in url:\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, f'../images/{food_name}/{food_name}_{png_counter}.png')\n",
    "                png_counter += 1\n",
    "                total_count += 1\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "    print(f'The total number of images for {food_name} is: {total_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of images for burgers is: 367\n",
      "The total number of images for hotdogs is: 341\n",
      "The total number of images for pizza is: 290\n",
      "The total number of images for tacos is: 301\n",
      "The total number of images for sushi is: 297\n"
     ]
    }
   ],
   "source": [
    "# Save all images by iterating over a for loop\n",
    "my_index = 0\n",
    "for food in food_url:\n",
    "    url_saver(food, food_list[my_index])\n",
    "    my_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Progress Update:** All the image data has been successfully collected and stored as .png and .jpg files! Incorrect images need to be removed manually to guarantee quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Incorrect Food Class Images\n",
    "Given the source of the images, not all images displayed the appropriate food class. To verify that the images belonged to the food class, all images were manually inspected, and inappropriate images were manually removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "Two major limitations in executing this project are explained below.\n",
    "\n",
    "The first limitation involves data collection. Originally, I had planned on utilizing a web API to scrape image data from Google Images. Due to the uncertainty of the legality of such an approach, I opted to use a less efficient method for collecting image data (i.e. via the pushshift.io Reddit API). Even if the intended approach were legal, I quickly ran into issues with funding and limitations on the number of requested API calls per month since most Google Image search APIs and proxies charge varying monthly fees depending on the plan [(source)](https://www.scraperapi.com/blog/best-google-image-search-apis-and-proxies/).\n",
    "\n",
    "The second limitation deals with the sample size. My initial goal was to build a CNN utilizing a minimum of 1,000 images per class. The problem with this is that the colored images not only took up a lot of storage on my local device in a way that slowed my processing speed but also proved too difficult to validate proper image classification. Since the subreddits frequently include images that are not actual images of food and require manual verification and removal, I reduced my sample size for each class to at least 200 images per class, which is less than ideal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
